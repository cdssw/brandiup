import time
import hmac
import hashlib
import base64
import requests
import urllib.parse
import os
import json
import logging
import re
from openai import OpenAI
from dotenv import load_dotenv

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

ADS_API_KEY = os.environ.get("NAVER_ADS_API_KEY")
ADS_SECRET_KEY = os.environ.get("NAVER_ADS_SECRET_KEY")
CUSTOMER_ID = os.environ.get("NAVER_CUSTOMER_ID")
SEARCH_CLIENT_ID = os.environ.get("NAVER_SEARCH_CLIENT_ID")
SEARCH_CLIENT_SECRET = os.environ.get("NAVER_SEARCH_CLIENT_SECRET")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

ADS_BASE_URL = "https://api.naver.com"

client = None
if OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)

def get_header(method, uri, api_key, secret_key, customer_id):
    timestamp = str(int(time.time() * 1000))
    signature = hmac.new(
        bytes(secret_key, "utf-8"),
        bytes(f"{timestamp}.{method}.{uri}", "utf-8"),
        hashlib.sha256
    ).digest()
    return {
        "X-Timestamp": timestamp,
        "X-API-KEY": api_key,
        "X-Customer": customer_id,
        "X-Signature": base64.b64encode(signature).decode("utf-8"),
    }

def get_blog_search_result(keyword):
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API"""
    if not SEARCH_CLIENT_ID: 
        return {"total": 0, "items": []}
    
    url = f"https://openapi.naver.com/v1/search/blog.json?query={urllib.parse.quote(keyword)}&display=3&sort=sim"
    headers = {
        "X-Naver-Client-Id": SEARCH_CLIENT_ID,
        "X-Naver-Client-Secret": SEARCH_CLIENT_SECRET
    }
    try:
        res = requests.get(url, headers=headers, timeout=3)
        if res.status_code == 200:
            data = res.json()
            return {"total": data.get("total", 0), "items": data.get("items", [])}
    except Exception as e:
        logger.warning(f"Blog search error: {e}")
    return {"total": 0, "items": []}

# ---------------------------------------------------------
# í‚¤ì›Œë“œ ì •ì œ í•¨ìˆ˜
# ---------------------------------------------------------
def sanitize_keyword(keyword):
    """í‚¤ì›Œë“œ ì •ì œ"""
    words = keyword.split()
    seen = set()
    unique_words = []
    for word in words:
        if word == "ë§›ì§‘" and any("ë§›ì§‘" in w for w in unique_words):
            continue
        if word not in seen:
            seen.add(word)
            unique_words.append(word)
    
    cleaned = " ".join(unique_words)
    if len(cleaned) > 30:
        cleaned = cleaned[:30]
    
    return cleaned.strip()

def validate_keyword(keyword):
    """í‚¤ì›Œë“œ ìœ íš¨ì„± ê²€ì‚¬"""
    if not keyword or len(keyword) < 2:
        return False
    if len(keyword) > 30:
        return False
    if not re.search(r'[ê°€-í£a-zA-Z]', keyword):
        return False
    return True

# ---------------------------------------------------------
# ì§€ì—­ ê³„ì¸µ êµ¬ì¡° íŒŒì‹±
# ---------------------------------------------------------
def parse_location_hierarchy(location_input):
    """ì§€ì—­ ê³„ì¸µ êµ¬ì¡° íŒŒì‹±"""
    cleaned = location_input.replace(" ì™¸ ", " ").replace("ê³³", "").strip()
    parts = cleaned.split()
    
    result = {
        'si': '',
        'gu': '',
        'dong_list': [],
        'search_locations': []
    }
    
    for part in parts:
        if 'ì‹œ' in part or 'êµ°' in part:
            result['si'] = part.replace('ì‹œ', '').replace('êµ°', '')
            break
    
    for part in parts:
        if 'êµ¬' in part and 'ì‹œ' not in part:
            result['gu'] = part
            break
    
    for part in parts:
        if any(suffix in part for suffix in ['ë™', 'ì', 'ë©´']):
            base = part.rstrip('0123456789').rstrip('ë™ìë©´ë¦¬ê°€')
            if base and len(base) >= 2:
                result['dong_list'].append(base)
            result['dong_list'].append(part)
    
    result['dong_list'] = list(dict.fromkeys(result['dong_list']))
    
    search_locs = []
    if result['si']:
        search_locs.append(result['si'])
    if result['gu']:
        search_locs.append(result['gu'])
        if result['si']:
            search_locs.append(f"{result['si']} {result['gu']}")
    if result['dong_list']:
        main_dong = result['dong_list'][0]
        search_locs.append(main_dong)
        if result['si']:
            search_locs.append(f"{result['si']} {main_dong}")
    
    result['search_locations'] = list(dict.fromkeys(search_locs))[:5]
    logger.info(f"ğŸ“ Location: {location_input} â†’ {result['search_locations']}")
    
    return result

# ---------------------------------------------------------
# [ENHANCED] AI í”„ë¡¬í”„íŠ¸ - ì „ë¬¸ ë§ˆì¼€í„° ê´€ì 
# ---------------------------------------------------------
def extract_keyword_materials(shop_name, products, category, tags, persona, location):
    """ì „ë¬¸ ë§ˆì¼€í„° ê´€ì ì˜ í‚¤ì›Œë“œ ì¬ë£Œ ì¶”ì¶œ"""
    if not client:
        logger.warning("OpenAI client not available")
        return None
    
    loc_hierarchy = parse_location_hierarchy(location)
    
    prompt = f"""ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë§ˆì¼€íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ê°€ê²Œ ì •ë³´]
- ìœ„ì¹˜: {location} (ì‹œ: {loc_hierarchy['si']}, êµ¬: {loc_hierarchy['gu']})
- ì—…ì¢…: {category}
- ë©”ë‰´: {products}
- íƒ€ê²Ÿ ê³ ê°: {persona}
- ê°€ê²Œ íŠ¹ì§•: {tags}

[ë¯¸ì…˜]
ì´ ê°€ê²Œê°€ ë„¤ì´ë²„ ê²€ìƒ‰ì—ì„œ ìƒìœ„ ë…¸ì¶œë˜ë„ë¡ í‚¤ì›Œë“œ ì „ëµì„ ì§œì„¸ìš”.

[ì¶œë ¥ í˜•ì‹ - JSON]
{{
    "core_menus": [
        // {products}ë³´ë‹¤ ê²€ìƒ‰ëŸ‰ ë§ì€ ëŒ€ì¤‘ì  ë©”ë‰´ëª… 4ê°œ
        // ì˜ˆ: "ë‹­êµ­ìˆ˜" â†’ ["ì¹¼êµ­ìˆ˜", "êµ­ìˆ˜", "íƒ•", "ë©´ìš”ë¦¬"]
    ],
    "category_words": [
        // {category} ëŒ€í‘œ í‚¤ì›Œë“œ 3ê°œ
        // ì˜ˆ: ["í•œì‹", "í•œì‹ë‹¹", "ë°¥ì§‘"]
    ],
    "target_intents": [
        // {persona}ì˜ ê²€ìƒ‰ ì˜ë„ 5ê°œ (ì „ë¬¸ê°€ ë¶„ì„)
        // ì˜ˆ: 30ëŒ€ ë‚¨ì„± â†’ ["ì ì‹¬", "íšŒì‹", "ì €ë…", "ìˆ ì•½ì†", "ê°€ì„±ë¹„"]
        // ì˜ˆ: 20ëŒ€ ì—¬ì„± â†’ ["ë°ì´íŠ¸", "ë¸ŒëŸ°ì¹˜", "ë¶„ìœ„ê¸°", "ì¸ìŠ¤íƒ€", "ì¹´í˜"]
    ],
    "situation_keywords": [
        // {tags} ê¸°ë°˜ ìƒí™©ë³„ í‚¤ì›Œë“œ 5ê°œ
        // ì˜ˆ: #í•´ì¥ â†’ ["í•´ì¥", "ìˆ ê¹¨ëŠ”", "ìˆ™ì·¨", "ì•„ì¹¨", "ì–¼í°"]
        // ì˜ˆ: #ë¹„ì˜¤ëŠ”ë‚  â†’ ["ë¹„ì˜¤ëŠ”ë‚ ", "ìš°ì¤‘", "ë¹„", "ë‚ ì”¨", "íë¦°ë‚ "]
    ],
    "purchase_triggers": [
        // êµ¬ë§¤ ì „í™˜ìœ¨ ë†’ì€ í‚¤ì›Œë“œ 4ê°œ (ì „ë¬¸ê°€ ë…¸í•˜ìš°)
        // ì˜ˆ: ["ì¶”ì²œ", "í›„ê¸°", "ë§›ì§‘", "ê°€ê¹Œìš´"]
    ],
    "insight": "{products.split(',')[0]}ë³´ë‹¤ core_menus[0]ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì´ìœ "
}}

[ì¤‘ìš”]
1. ëª¨ë“  í‚¤ì›Œë“œëŠ” ë‹¨ì¼ ëª…ì‚¬ë§Œ
2. ì‹¤ì œ ê²€ìƒ‰ë  ë§Œí•œ ë‹¨ì–´ë§Œ
3. ì¤‘ë³µ ê¸ˆì§€
4. ì§€ì—­ íŠ¹ì„± ê³ ë ¤
"""
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.7,
            max_tokens=800
        )
        result = json.loads(response.choices[0].message.content)
        logger.info(f"âœ… AI Materials: {json.dumps(result, ensure_ascii=False)}")
        return result
    except Exception as e:
        logger.error(f"âŒ AI Error: {e}")
        return None

# ---------------------------------------------------------
# [REVOLUTIONARY] íƒ€ì…ë³„ ê· í˜• í‚¤ì›Œë“œ ìƒì„±
# ---------------------------------------------------------
def generate_and_validate_keywords(location, products, tags_input, materials):
    """
    ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ í‚¤ì›Œë“œ ìƒì„±
    - íƒ€ì…ë³„ ì¿¼í„° ì‹œìŠ¤í…œ
    - ê· í˜•ì¡íŒ API í˜¸ì¶œ
    """
    if not materials:
        logger.warning("Materials not available")
        return create_empty_report()
    
    # 1. ì§€ì—­ ê³„ì¸µ íŒŒì‹±
    loc_hierarchy = parse_location_hierarchy(location)
    search_locations = loc_hierarchy['search_locations']
    
    if not search_locations:
        return create_empty_report()
    
    main_menu = products.split(",")[0].strip() if products else ""
    all_menus = [m.strip() for m in products.split(",") if m.strip()][:3]
    tags = [t.replace("#", "").strip() for t in tags_input.split() if t.strip()]
    
    # 2. AI ì¬ë£Œ
    core_menus = materials.get("core_menus", [main_menu])[:4]
    category_words = materials.get("category_words", ["ë§›ì§‘"])[:3]
    target_intents = materials.get("target_intents", ["ì ì‹¬", "ì €ë…"])[:5]
    situation_keywords = materials.get("situation_keywords", tags)[:5]
    purchase_triggers = materials.get("purchase_triggers", ["ì¶”ì²œ", "í›„ê¸°"])[:4]
    
    # 3. íƒ€ì…ë³„ í‚¤ì›Œë“œ ìƒì„± (ì¿¼í„° ì‹œìŠ¤í…œ)
    keyword_pool = {
        'A_Core': [],      # ë©”ì¸ íƒ€ê²Ÿ (ê²€ìƒ‰ëŸ‰ ë†’ìŒ)
        'C_Target': [],    # íƒ€ê²Ÿ ë§ì¶¤ (ì „í™˜ìœ¨ ë†’ìŒ)
        'D_Situation': []  # ìƒí™© í‚¤ì›Œë“œ (ë¡±í…Œì¼)
    }
    
    # [Type A] ë©”ì¸ íƒ€ê²Ÿ í‚¤ì›Œë“œ ìƒì„±
    for loc in search_locations[:2]:
        for menu in core_menus[:3]:
            keyword_pool['A_Core'].append(f"{loc} {menu}")
            keyword_pool['A_Core'].append(f"{loc} {menu} ë§›ì§‘")
        
        for cat in category_words:
            if "ë§›ì§‘" not in cat:
                keyword_pool['A_Core'].append(f"{loc} {cat}")
                keyword_pool['A_Core'].append(f"{loc} {cat} ë§›ì§‘")
            else:
                keyword_pool['A_Core'].append(f"{loc} {cat}")
    
    # [Type C] íƒ€ê²Ÿ ë§ì¶¤ í‚¤ì›Œë“œ ìƒì„± (ì „ë¬¸ê°€ ë…¸í•˜ìš°)
    primary_loc = search_locations[0]
    
    for intent in target_intents:
        # ì˜ë„ + ë©”ë‰´
        for menu in core_menus[:2]:
            keyword_pool['C_Target'].append(f"{primary_loc} {intent} {menu}")
        
        # ì˜ë„ + ì¹´í…Œê³ ë¦¬
        keyword_pool['C_Target'].append(f"{primary_loc} {intent} {category_words[0]}")
        keyword_pool['C_Target'].append(f"{primary_loc} {intent} ë§›ì§‘")
    
    # êµ¬ë§¤ ì „í™˜ í‚¤ì›Œë“œ
    for trigger in purchase_triggers:
        keyword_pool['C_Target'].append(f"{primary_loc} {main_menu} {trigger}")
        keyword_pool['C_Target'].append(f"{primary_loc} {category_words[0]} {trigger}")
    
    # [Type D] ìƒí™© í‚¤ì›Œë“œ ìƒì„± (ë¡±í…Œì¼ ì „ëµ)
    for situation in situation_keywords:
        # ìƒí™© ë‹¨ë…
        keyword_pool['D_Situation'].append(f"{primary_loc} {situation}")
        keyword_pool['D_Situation'].append(f"{primary_loc} {situation} ë§›ì§‘")
        
        # ìƒí™© + ë©”ë‰´
        if main_menu:
            keyword_pool['D_Situation'].append(f"{primary_loc} {situation} {main_menu}")
        
        # ìƒí™© + ì¹´í…Œê³ ë¦¬
        keyword_pool['D_Situation'].append(f"{primary_loc} {situation} {category_words[0]}")
    
    # íƒœê·¸ ê¸°ë°˜ ì¶”ê°€
    for tag in tags[:3]:
        if tag not in situation_keywords:
            keyword_pool['D_Situation'].append(f"{primary_loc} {tag}")
            keyword_pool['D_Situation'].append(f"{primary_loc} {tag} ë§›ì§‘")
    
    # 4. í‚¤ì›Œë“œ ì •ì œ (íƒ€ì…ë³„)
    for kw_type in keyword_pool:
        cleaned = []
        seen = set()
        
        for kwd in keyword_pool[kw_type]:
            sanitized = sanitize_keyword(kwd)
            if validate_keyword(sanitized) and sanitized not in seen:
                cleaned.append(sanitized)
                seen.add(sanitized)
        
        keyword_pool[kw_type] = cleaned
    
    # 5. íƒ€ì…ë³„ ì¿¼í„° ì ìš© (ê· í˜•ì¡íŒ ì„ íƒ)
    selected_candidates = []
    
    # A_Core: 5ê°œ
    for kwd in keyword_pool['A_Core'][:5]:
        selected_candidates.append({
            "kwd": kwd,
            "type": "A_Core",
            "priority": 100
        })
    
    # C_Target: 6ê°œ (ì¤‘ìš”!)
    for kwd in keyword_pool['C_Target'][:6]:
        selected_candidates.append({
            "kwd": kwd,
            "type": "C_Target",
            "priority": 95  # ìš°ì„ ìˆœìœ„ ìƒí–¥!
        })
    
    # D_Situation: 5ê°œ
    for kwd in keyword_pool['D_Situation'][:5]:
        selected_candidates.append({
            "kwd": kwd,
            "type": "D_Situation",
            "priority": 90  # ìš°ì„ ìˆœìœ„ ìƒí–¥!
        })
    
    logger.info(f"ğŸ¯ Balanced Selection: A_Core={len([c for c in selected_candidates if c['type']=='A_Core'])}, C_Target={len([c for c in selected_candidates if c['type']=='C_Target'])}, D_Situation={len([c for c in selected_candidates if c['type']=='D_Situation'])}")
    
    for i, c in enumerate(selected_candidates[:10], 1):
        logger.info(f"   {i}. [{c['type']}] {c['kwd']}")
    
    # 6. API ê²€ì¦ (íƒ€ì…ë³„ ê· í˜• ìœ ì§€)
    validated_keywords = []
    
    if ADS_API_KEY:
        validated_keywords = validate_with_balanced_api(selected_candidates)
    
    # 7. í´ë°± ì „ëµ (íƒ€ì…ë³„ ìµœì†Œ ë³´ì¥)
    validated_keywords = ensure_minimum_keywords(
        validated_keywords,
        selected_candidates,
        search_locations,
        core_menus,
        main_menu
    )
    
    # 8. ê²°ê³¼ ë¶„ë¥˜
    final_report = classify_keywords(
        validated_keywords,
        materials,
        search_locations,
        core_menus,
        tags,
        main_menu
    )
    
    return final_report

def validate_with_balanced_api(candidates):
    """
    íƒ€ì…ë³„ ê· í˜•ì„ ìœ ì§€í•˜ë©´ì„œ API í˜¸ì¶œ
    ê° íƒ€ì…ì—ì„œ ìµœì†Œ 1-2ê°œì”© ê²€ì¦
    """
    validated = []
    api_call_count = 0
    MAX_API_CALLS = 5  # 3â†’5ë¡œ ì¦ê°€
    
    # íƒ€ì…ë³„ ê·¸ë£¹í™”
    type_groups = {
        'A_Core': [c for c in candidates if c['type'] == 'A_Core'],
        'C_Target': [c for c in candidates if c['type'] == 'C_Target'],
        'D_Situation': [c for c in candidates if c['type'] == 'D_Situation']
    }
    
    # íƒ€ì…ë³„ë¡œ ë²ˆê°ˆì•„ê°€ë©° API í˜¸ì¶œ
    batch_queue = []
    
    # ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ë°°ì¹˜ êµ¬ì„±
    max_per_type = 2  # ê° íƒ€ì…ì—ì„œ 2ê°œì”©
    for i in range(max_per_type):
        for kw_type in ['A_Core', 'C_Target', 'D_Situation']:
            if i < len(type_groups[kw_type]):
                batch_queue.append(type_groups[kw_type][i])
    
    # ë‚¨ì€ ê²ƒë“¤ ì¶”ê°€
    for kw_type in ['A_Core', 'C_Target', 'D_Situation']:
        remaining = type_groups[kw_type][max_per_type:]
        batch_queue.extend(remaining[:2])  # ê°ê° 2ê°œì”© ë”
    
    logger.info(f"ğŸ”„ API Queue: {len(batch_queue)} candidates ({len([c for c in batch_queue if c['type']=='A_Core'])} Core, {len([c for c in batch_queue if c['type']=='C_Target'])} Target, {len([c for c in batch_queue if c['type']=='D_Situation'])} Situation)")
    
    # API í˜¸ì¶œ
    for i in range(0, len(batch_queue), 5):
        if api_call_count >= MAX_API_CALLS:
            break
        
        chunk = batch_queue[i:i+5]
        hint_str = ",".join([c['kwd'].replace(" ", "") for c in chunk])
        
        try:
            headers = get_header("GET", "/keywordstool", ADS_API_KEY, ADS_SECRET_KEY, CUSTOMER_ID)
            params = {"hintKeywords": hint_str, "showDetail": "1"}
            
            logger.info(f"ğŸ“¡ API Call #{api_call_count + 1}: {[(c['type'], c['kwd']) for c in chunk]}")
            res = requests.get(ADS_BASE_URL + "/keywordstool", params=params, headers=headers, timeout=5)
            
            if res.status_code == 200:
                api_data = res.json().get("keywordList", [])
                logger.info(f"   âœ… Success: {len(api_data)} results")
                
                for item in api_data:
                    rel_kwd = item.get('relKeyword', '')
                    pc = int(item.get('monthlyPcQcCnt', 0)) if str(item.get('monthlyPcQcCnt', 0)).isdigit() else 0
                    mo = int(item.get('monthlyMobileQcCnt', 0)) if str(item.get('monthlyMobileQcCnt', 0)).isdigit() else 0
                    comp_idx = item.get('compIdx', 'low')
                    
                    total_volume = pc + mo
                    
                    if total_volume >= 10:
                        matched = None
                        for c in chunk:
                            c_clean = c['kwd'].replace(" ", "")
                            rel_clean = rel_kwd.replace(" ", "")
                            if c_clean in rel_clean or rel_clean in c_clean:
                                matched = c
                                break
                        
                        if not matched:
                            matched = {"type": "E_Related", "priority": 50}
                        
                        comp_score = {'low': 100, 'medium': 50, 'high': 20}.get(comp_idx, 50)
                        score = (total_volume * 0.5) + comp_score + matched.get('priority', 0)
                        
                        validated.append({
                            "keyword": rel_kwd,
                            "volume": total_volume,
                            "pc": pc,
                            "mobile": mo,
                            "competition": comp_idx,
                            "type": matched['type'],
                            "score": score
                        })
                        logger.info(f"      â†’ [{matched['type']}] {rel_kwd}: {total_volume:,}")
            else:
                try:
                    error_body = res.json()
                    logger.error(f"   âŒ API Error {res.status_code}: {error_body}")
                except:
                    logger.error(f"   âŒ API Error {res.status_code}")
            
            api_call_count += 1
            time.sleep(0.2)
            
        except Exception as e:
            logger.error(f"âŒ API Exception: {str(e)}")
            api_call_count += 1
    
    # íƒ€ì…ë³„ ê²€ì¦ ê°œìˆ˜ í™•ì¸
    type_counts = {}
    for kw in validated:
        kw_type = kw['type']
        type_counts[kw_type] = type_counts.get(kw_type, 0) + 1
    
    logger.info(f"ğŸ“Š API Validation: Total={len(validated)}, {type_counts}")
    
    return validated

def ensure_minimum_keywords(validated, all_candidates, locations, menus, main_menu):
    """
    íƒ€ì…ë³„ ìµœì†Œ ê°œìˆ˜ ë³´ì¥
    - A_Core: ìµœì†Œ 4ê°œ
    - C_Target: ìµœì†Œ 5ê°œ
    - D_Situation: ìµœì†Œ 4ê°œ
    """
    type_counts = {}
    for kw in validated:
        kw_type = kw['type']
        type_counts[kw_type] = type_counts.get(kw_type, 0) + 1
    
    logger.info(f"ğŸ” Current counts: {type_counts}")
    
    # ìµœì†Œ ëª©í‘œ
    min_targets = {
        'A_Core': 4,
        'C_Target': 5,
        'D_Situation': 4
    }
    
    existing_kwds = set([kw['keyword'] for kw in validated])
    
    for kw_type, min_count in min_targets.items():
        current_count = type_counts.get(kw_type, 0)
        shortage = min_count - current_count
        
        if shortage > 0:
            logger.info(f"âš ï¸ {kw_type} shortage: {shortage}, adding fallback")
            
            # í•´ë‹¹ íƒ€ì…ì˜ í›„ë³´ ì¤‘ ì•„ì§ ê²€ì¦ ì•ˆëœ ê²ƒ
            type_candidates = [c for c in all_candidates if c['type'] == kw_type and c['kwd'] not in existing_kwds]
            
            for c in type_candidates[:shortage]:
                # ì§€ì—­ í¬ê¸°ì— ë”°ë¥¸ ì¶”ì • ê²€ìƒ‰ëŸ‰
                base_volume = 200
                if locations[0] in c['kwd']:
                    base_volume = 250
                if len(locations) > 1 and locations[1] in c['kwd']:
                    base_volume = 150
                
                # íƒ€ì…ë³„ ì¶”ì • ê²€ìƒ‰ëŸ‰ ì¡°ì •
                if kw_type == 'C_Target':
                    base_volume = int(base_volume * 0.7)  # íƒ€ê²Ÿ í‚¤ì›Œë“œëŠ” ì•½ê°„ ë‚®ìŒ
                elif kw_type == 'D_Situation':
                    base_volume = int(base_volume * 0.5)  # ìƒí™© í‚¤ì›Œë“œëŠ” ë” ë‚®ìŒ
                
                validated.append({
                    "keyword": c['kwd'],
                    "volume": base_volume,
                    "pc": int(base_volume * 0.3),
                    "mobile": int(base_volume * 0.7),
                    "competition": "low",
                    "type": kw_type,
                    "score": c['priority'],
                    "is_estimated": True
                })
                existing_kwds.add(c['kwd'])
                logger.info(f"   + [{kw_type}] {c['kwd']} (~{base_volume})")
    
    return validated

def classify_keywords(validated_keywords, materials, locations, menus, tags, main_menu):
    """í‚¤ì›Œë“œ ë¶„ë¥˜ ë° ìµœì¢… ë¦¬í¬íŠ¸"""
    final_report = {
        "insight": materials.get("insight", "ì „ë¬¸ê°€ ë¶„ì„ ì™„ë£Œ"),
        "main_keywords": [],
        "detail_keywords": [],
        "related_keywords": [],
        "content_ideas": [],
        "debug_info": {
            "total_validated": len(validated_keywords),
            "locations_used": locations
        }
    }
    
    # ì¤‘ë³µ ì œê±°
    unique_validated = {}
    for kw in validated_keywords:
        key = kw['keyword']
        if key not in unique_validated or unique_validated[key]['score'] < kw['score']:
            unique_validated[key] = kw
    
    validated_keywords = list(unique_validated.values())
    validated_keywords.sort(key=lambda x: x['score'], reverse=True)
    
    # ë¶„ë¥˜
    for kw in validated_keywords:
        kw_type = kw['type']
        if kw_type in ['A_Core', 'B_Local']:
            final_report['main_keywords'].append(kw)
        elif kw_type in ['C_Target', 'D_Situation']:
            final_report['detail_keywords'].append(kw)
        elif kw_type == 'E_Related':
            final_report['related_keywords'].append(kw)
    
    # ê°œìˆ˜ ì œí•œ
    final_report['main_keywords'] = final_report['main_keywords'][:10]
    final_report['detail_keywords'] = final_report['detail_keywords'][:12]  # ì„¸ë¶€ í‚¤ì›Œë“œ ê°œìˆ˜ ì¦ê°€
    final_report['related_keywords'] = final_report['related_keywords'][:5]
    
    # ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì½˜í…ì¸  ì•„ì´ë””ì–´
    ideas = []
    
    if final_report['main_keywords']:
        top_kw = final_report['main_keywords'][0]
        vol_text = f"{top_kw['volume']:,}ê±´" if not top_kw.get('is_estimated') else f"~{top_kw['volume']:,}ê±´"
        ideas.append(
            f"ğŸ“Š ë©”ì¸ SEO ë¸”ë¡œê·¸: \"{top_kw['keyword']} ë² ìŠ¤íŠ¸ 5 - í˜„ì§€ì¸ ì¶”ì²œ\" (ì›” {vol_text}, ë†’ì€ ìœ ì…)"
        )
    
    if final_report['detail_keywords']:
        detail_kw = final_report['detail_keywords'][0]
        vol_text = f"{detail_kw['volume']:,}ê±´" if not detail_kw.get('is_estimated') else f"~{detail_kw['volume']:,}ê±´"
        ideas.append(
            f"ğŸ¯ ì „í™˜ ìµœì í™” ì½˜í…ì¸ : \"{detail_kw['keyword']} ì†”ì§ í›„ê¸°\" (ì›” {vol_text}, ë†’ì€ ì „í™˜ìœ¨)"
        )
    
    if tags and menus:
        ideas.append(
            f"ğŸ’¡ ìƒí™© ê³µê° ì½˜í…ì¸ : \"{tags[0]} ë•ŒëŠ” {locations[0]} {menus[0]}ê°€ ìµœê³ ì¸ ì´ìœ \" (ë°”ì´ëŸ´ ìœ ë„)"
        )
    
    if final_report['detail_keywords'] and len(final_report['detail_keywords']) > 1:
        detail_kw2 = final_report['detail_keywords'][1]
        ideas.append(
            f"ğŸ”¥ íƒ€ê²Ÿ ë§ì¶¤ ì½˜í…ì¸ : \"{detail_kw2['keyword']} ê°€ê¸° ì „ ê¼­ ì•Œì•„ì•¼ í•  ê²ƒ\" (ì¬ë°©ë¬¸ ìœ ë„)"
        )
    
    final_report['content_ideas'] = ideas
    
    logger.info(f"ğŸ“Š Final Report: Main={len(final_report['main_keywords'])}, Detail={len(final_report['detail_keywords'])}, Related={len(final_report['related_keywords'])}")
    
    return final_report

def create_empty_report():
    """ë¹ˆ ë¦¬í¬íŠ¸"""
    return {
        "insight": "í‚¤ì›Œë“œ ë¶„ì„ ë°ì´í„° ë¶€ì¡±",
        "main_keywords": [],
        "detail_keywords": [],
        "related_keywords": [],
        "content_ideas": [],
        "debug_info": {}
    }