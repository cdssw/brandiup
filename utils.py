import time
import hmac
import hashlib
import base64
import requests
import urllib.parse
import os
import json
import logging
import re
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

ADS_API_KEY = os.environ.get("NAVER_ADS_API_KEY")
ADS_SECRET_KEY = os.environ.get("NAVER_ADS_SECRET_KEY")
CUSTOMER_ID = os.environ.get("NAVER_CUSTOMER_ID")
SEARCH_CLIENT_ID = os.environ.get("NAVER_SEARCH_CLIENT_ID")
SEARCH_CLIENT_SECRET = os.environ.get("NAVER_SEARCH_CLIENT_SECRET")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

ADS_BASE_URL = "https://api.naver.com"

client = None
if OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)

def get_header(method, uri, api_key, secret_key, customer_id):
    timestamp = str(int(time.time() * 1000))
    signature = hmac.new(
        bytes(secret_key, "utf-8"),
        bytes(f"{timestamp}.{method}.{uri}", "utf-8"),
        hashlib.sha256
    ).digest()
    return {
        "X-Timestamp": timestamp,
        "X-API-KEY": api_key,
        "X-Customer": customer_id,
        "X-Signature": base64.b64encode(signature).decode("utf-8"),
    }

# ---------------------------------------------------------
# [NEW] ê³„ì ˆ ê°ì§€
# ---------------------------------------------------------
def get_current_season():
    """í˜„ì¬ ê³„ì ˆ ê°ì§€"""
    month = datetime.now().month
    if month in [3, 4, 5]:
        return "ë´„"
    elif month in [6, 7, 8]:
        return "ì—¬ë¦„"
    elif month in [9, 10, 11]:
        return "ê°€ì„"
    else:
        return "ê²¨ìš¸"

def get_seasonal_keywords(category, season):
    """ê³„ì ˆë³„ í‚¤ì›Œë“œ ë§µ"""
    seasonal_map = {
        "ë´„": {
            "í•œì‹": ["ë´„ë‚˜ë¬¼", "ì‘¥", "ëƒ‰ì´", "ë´„ì² ", "ë”°ëœ»í•œ", "ë´„ë§ì´"],
            "êµ­ìˆ˜/ë©´ìš”ë¦¬": ["ë¹„ë¹”êµ­ìˆ˜", "ìŸë°˜êµ­ìˆ˜", "ë´„ì² ", "ì‹œì›í•œ"],
            "ë³´ì–‘ì‹": ["ì¶˜ê³¤ì¦", "í”¼ë¡œíšŒë³µ", "í™œë ¥", "ë´„ë³´ì–‘"],
            "ì¹´í˜/ë””ì €íŠ¸": ["ë²šê½ƒ", "í…Œë¼ìŠ¤", "ì•¼ì™¸", "ë´„ì¹´í˜", "ê½ƒêµ¬ê²½"],
            "default": ["ë´„", "ë´„ì² ", "ë”°ëœ»í•œ"]
        },
        "ì—¬ë¦„": {
            "í•œì‹": ["ëƒ‰ë©´", "ì½©êµ­ìˆ˜", "ëƒ‰êµ­", "ì‹œì›í•œ", "ì—¬ë¦„"],
            "êµ­ìˆ˜/ë©´ìš”ë¦¬": ["ëƒ‰ë©´", "ë¹„ë¹”êµ­ìˆ˜", "ì½©êµ­ìˆ˜", "ì—´ë¬´êµ­ìˆ˜", "ì‹œì›í•œ"],
            "ë³´ì–‘ì‹": ["ì‚¼ê³„íƒ•", "ë³´ì‹ ", "ì—¬ë¦„ë³´ì–‘", "ê¸°ë ¥íšŒë³µ", "ì˜ì–‘"],
            "ì¹´í˜/ë””ì €íŠ¸": ["ë¹™ìˆ˜", "ì•„ì´ìŠ¤", "ì‹œì›í•œ", "ì—¬ë¦„"],
            "default": ["ì—¬ë¦„", "ì‹œì›í•œ", "ë”ìœ„"]
        },
        "ê°€ì„": {
            "í•œì‹": ["ì „", "ë§‰ê±¸ë¦¬", "êµ­ë°¥", "ë”°ëœ»í•œ", "ê°€ì„"],
            "êµ­ìˆ˜/ë©´ìš”ë¦¬": ["ì¹¼êµ­ìˆ˜", "ë”°ëœ»í•œ", "ì–¼í°í•œ", "ê°€ì„"],
            "ë³´ì–‘ì‹": ["ë³´ì–‘", "ì˜ì–‘", "í™˜ì ˆê¸°", "ê¸°ë ¥"],
            "ì¹´í˜/ë””ì €íŠ¸": ["ë‹¨í’", "ê°€ì„", "ë”°ëœ»í•œ", "ê°€ì„ì¹´í˜"],
            "default": ["ê°€ì„", "í™˜ì ˆê¸°", "ë”°ëœ»í•œ"]
        },
        "ê²¨ìš¸": {
            "í•œì‹": ["êµ­ë°¥", "í•´ì¥êµ­", "ê³°íƒ•", "ì–¼í°í•œ", "ëœ¨ëˆí•œ", "ê²¨ìš¸"],
            "êµ­ìˆ˜/ë©´ìš”ë¦¬": ["ì¹¼êµ­ìˆ˜", "ëœ¨ëˆí•œ", "ì–¼í°í•œ", "í•´ì¥", "ê²¨ìš¸"],
            "ë³´ì–‘ì‹": ["ë³´ì‹ ", "ëª¸ë³´ì‹ ", "ë”°ëœ»í•œ", "ì˜ì–‘", "ê²¨ìš¸ë³´ì–‘"],
            "ì¹´í˜/ë””ì €íŠ¸": ["ë”°ëœ»í•œ", "ê²¨ìš¸", "í•«ì´ˆì½”"],
            "default": ["ê²¨ìš¸", "ëœ¨ëˆí•œ", "ë”°ëœ»í•œ"]
        }
    }
    
    return seasonal_map.get(season, {}).get(category, seasonal_map[season]["default"])

# ---------------------------------------------------------
# [NEW] ë¸”ë¡œê·¸ ê²€ìƒ‰ ë° ê²½ìŸì‚¬ ë¶„ì„
# ---------------------------------------------------------
def get_blog_search_result(keyword):
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API - ê°œì„ """
    if not SEARCH_CLIENT_ID: 
        return {"total": 0, "items": []}
    
    url = f"https://openapi.naver.com/v1/search/blog.json?query={urllib.parse.quote(keyword)}&display=10&sort=sim"
    headers = {
        "X-Naver-Client-Id": SEARCH_CLIENT_ID,
        "X-Naver-Client-Secret": SEARCH_CLIENT_SECRET
    }
    try:
        res = requests.get(url, headers=headers, timeout=3)
        if res.status_code == 200:
            data = res.json()
            return {
                "total": data.get("total", 0), 
                "items": data.get("items", [])
            }
    except Exception as e:
        logger.warning(f"Blog search error: {e}")
    return {"total": 0, "items": []}

def analyze_competitor_blogs(keyword):
    """
    ê²½ìŸì‚¬ ë¸”ë¡œê·¸ ë¶„ì„
    - ìƒìœ„ ë…¸ì¶œ ë¸”ë¡œê·¸ ë¶„ì„
    - í¬ìŠ¤íŒ… ì „ëµ íŒŒì•…
    """
    blog_data = get_blog_search_result(keyword)
    
    if blog_data['total'] == 0:
        return {
            'total_posts': 0,
            'top_competitors': [],
            'competition_level': 'low',
            'strategy_insight': f"'{keyword}' í‚¤ì›Œë“œëŠ” ì•„ì§ ê²½ìŸì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. ì§€ê¸ˆ ì‹œì‘í•˜ë©´ ì„ ì  ê°€ëŠ¥í•©ë‹ˆë‹¤!"
        }
    
    top_blogs = []
    for idx, item in enumerate(blog_data['items'][:3], 1):
        # HTML íƒœê·¸ ì œê±°
        title = re.sub(r'<[^>]+>', '', item.get('title', ''))
        description = re.sub(r'<[^>]+>', '', item.get('description', ''))
        
        top_blogs.append({
            'rank': idx,
            'title': title[:50],
            'blogger': item.get('bloggername', 'ì•Œ ìˆ˜ ì—†ìŒ'),
            'date': item.get('postdate', ''),
            'link': item.get('link', '')
        })
    
    # ê²½ìŸ ê°•ë„ íŒë‹¨
    if blog_data['total'] < 50:
        competition_level = 'low'
        strategy = f"ì´ {blog_data['total']}ê°œì˜ í¬ìŠ¤íŒ…ë§Œ ìˆì–´ ê²½ìŸì´ ì•½í•©ë‹ˆë‹¤. ë¸”ë¡œê·¸ 20-30ê°œë¡œ ìƒìœ„ ë…¸ì¶œ ê°€ëŠ¥!"
    elif blog_data['total'] < 500:
        competition_level = 'medium'
        strategy = f"ì´ {blog_data['total']}ê°œ í¬ìŠ¤íŒ…. ì¤‘ê°„ ê²½ìŸì…ë‹ˆë‹¤. ê¾¸ì¤€í•œ ë¸”ë¡œê·¸ + ì¸ìŠ¤íƒ€ ì—°ë™ìœ¼ë¡œ 3ê°œì›” ë‚´ ìƒìœ„ê¶Œ ê°€ëŠ¥!"
    else:
        competition_level = 'high'
        strategy = f"ì´ {blog_data['total']:,}ê°œë¡œ ê²½ìŸì´ ì¹˜ì—´í•©ë‹ˆë‹¤. ë¡±í…Œì¼ í‚¤ì›Œë“œ(ì˜ˆ: '{keyword} í›„ê¸°') ê³µëµ ì¶”ì²œ!"
    
    return {
        'total_posts': blog_data['total'],
        'top_competitors': top_blogs,
        'competition_level': competition_level,
        'strategy_insight': strategy
    }

# ---------------------------------------------------------
# í‚¤ì›Œë“œ ì •ì œ í•¨ìˆ˜
# ---------------------------------------------------------
def sanitize_keyword(keyword):
    """í‚¤ì›Œë“œ ì •ì œ"""
    words = keyword.split()
    seen = set()
    unique_words = []
    for word in words:
        if word == "ë§›ì§‘" and any("ë§›ì§‘" in w for w in unique_words):
            continue
        if word not in seen:
            seen.add(word)
            unique_words.append(word)
    
    cleaned = " ".join(unique_words)
    if len(cleaned) > 30:
        cleaned = cleaned[:30]
    
    return cleaned.strip()

def validate_keyword(keyword):
    """í‚¤ì›Œë“œ ìœ íš¨ì„± ê²€ì‚¬"""
    if not keyword or len(keyword) < 2:
        return False
    if len(keyword) > 30:
        return False
    if not re.search(r'[ê°€-í£a-zA-Z]', keyword):
        return False
    return True

# ---------------------------------------------------------
# ì§€ì—­ ê³„ì¸µ êµ¬ì¡° íŒŒì‹±
# ---------------------------------------------------------
def parse_location_hierarchy(location_input):
    """ì§€ì—­ ê³„ì¸µ êµ¬ì¡° íŒŒì‹±"""
    cleaned = location_input.replace(" ì™¸ ", " ").replace("ê³³", "").strip()
    parts = cleaned.split()
    
    result = {
        'si': '',
        'gu': '',
        'dong_list': [],
        'search_locations': []
    }
    
    for part in parts:
        if 'ì‹œ' in part or 'êµ°' in part:
            result['si'] = part.replace('ì‹œ', '').replace('êµ°', '')
            break
    
    for part in parts:
        if 'êµ¬' in part and 'ì‹œ' not in part:
            result['gu'] = part
            break
    
    for part in parts:
        if any(suffix in part for suffix in ['ë™', 'ì', 'ë©´']):
            base = part.rstrip('0123456789').rstrip('ë™ìë©´ë¦¬ê°€')
            if base and len(base) >= 2:
                result['dong_list'].append(base)
            result['dong_list'].append(part)
    
    result['dong_list'] = list(dict.fromkeys(result['dong_list']))
    
    search_locs = []
    if result['si']:
        search_locs.append(result['si'])
    if result['gu']:
        search_locs.append(result['gu'])
        if result['si']:
            search_locs.append(f"{result['si']} {result['gu']}")
    if result['dong_list']:
        main_dong = result['dong_list'][0]
        search_locs.append(main_dong)
        if result['si']:
            search_locs.append(f"{result['si']} {main_dong}")
    
    result['search_locations'] = list(dict.fromkeys(search_locs))[:5]
    logger.info(f"ğŸ“ Location: {location_input} â†’ {result['search_locations']}")
    
    return result

# ---------------------------------------------------------
# [ENHANCED] AI í”„ë¡¬í”„íŠ¸ - ê³„ì ˆ + ì „ëµ ë°˜ì˜
# ---------------------------------------------------------
def extract_keyword_materials(shop_name, products, category, tags, persona, location):
    """
    ì „ë¬¸ ë§ˆì¼€í„° ê´€ì ì˜ í‚¤ì›Œë“œ ì¬ë£Œ ì¶”ì¶œ
    - ëª¨ë“  ë©”ë‰´ ë°˜ì˜
    - ê³„ì ˆì„± ë°˜ì˜
    - íƒ€ê²Ÿì¸µ íŠ¹ì„± ë°˜ì˜
    """
    if not client:
        logger.warning("OpenAI client not available")
        return None
    
    loc_hierarchy = parse_location_hierarchy(location)
    current_season = get_current_season()
    seasonal_kws = get_seasonal_keywords(category, current_season)
    
    # ë©”ë‰´ íŒŒì‹±
    menu_list = [m.strip() for m in products.split(",") if m.strip()]
    main_menu = menu_list[0] if menu_list else products
    all_menus_str = ", ".join(menu_list)
    
    prompt = f"""ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë§ˆì¼€íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ê°€ê²Œ ì •ë³´]
- ìœ„ì¹˜: {location} (ì‹œ: {loc_hierarchy['si']}, êµ¬: {loc_hierarchy['gu']})
- ì—…ì¢…: {category}
- ëŒ€í‘œ ë©”ë‰´: {all_menus_str}
  * ë©”ì¸: {main_menu}
  * ì „ì²´: {menu_list}
- íƒ€ê²Ÿ ê³ ê°: {persona}
- ê°€ê²Œ íŠ¹ì§•: {tags}

[ê³„ì ˆ ì •ë³´]
- í˜„ì¬ ê³„ì ˆ: {current_season}
- ê³„ì ˆ í‚¤ì›Œë“œ: {seasonal_kws}

[ë¯¸ì…˜]
ì§€ê¸ˆì€ **{current_season}**ì…ë‹ˆë‹¤. ì´ ê³„ì ˆì— {persona}ê°€ ê²€ìƒ‰í•  ë§Œí•œ í‚¤ì›Œë“œë¥¼ ì°¾ìœ¼ì„¸ìš”.
ëª¨ë“  ë©”ë‰´ë¥¼ ê³ ë ¤í•˜ë˜, ê³„ì ˆì„±ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì„¸ìš”.

[ì¶œë ¥ - JSON]
{{
    "actual_menus": [
        // ì…ë ¥ëœ ì‹¤ì œ ë©”ë‰´ ê·¸ëŒ€ë¡œ (ìµœëŒ€ 5ê°œ)
        // ì˜ˆ: ["ë‹­êµ­ìˆ˜", "ë‹­ë„ë¦¬íƒ•", "ë‹­ê³°íƒ•"]
    ],
    "expanded_menus": [
        // ê° ë©”ë‰´ì˜ ìœ ì‚¬ì–´ + ê³„ì ˆ ê³ ë ¤ (6ê°œ)
        // ê²¨ìš¸: "ë‹­êµ­ìˆ˜" â†’ "ë‹­ì¹¼êµ­ìˆ˜", "ëœ¨ëˆí•œ êµ­ìˆ˜"
        // ì—¬ë¦„: "ë‹­êµ­ìˆ˜" â†’ "ë¹„ë¹”êµ­ìˆ˜", "ì‹œì›í•œ êµ­ìˆ˜"
    ],
    "seasonal_keywords": [
        // {current_season}ì— ë§ëŠ” í‚¤ì›Œë“œ 5ê°œ
        // ì˜ˆ: ê²¨ìš¸ - ["ëœ¨ëˆí•œ", "í•´ì¥", "ì–¼í°í•œ", "ë”°ëœ»í•œ", "ê²¨ìš¸"]
    ],
    "target_intents": [
        // {persona}ì˜ ê²€ìƒ‰ ì˜ë„ 5ê°œ (ê³„ì ˆ ë°˜ì˜)
        // ì˜ˆ: ê²¨ìš¸ + 30ëŒ€ ë‚¨ì„± â†’ ["ì ì‹¬", "í•´ì¥", "íšŒì‹", "ìˆ ì•½ì†", "ë”°ëœ»í•œ"]
    ],
    "situation_keywords": [
        // {tags} + ê³„ì ˆ ì¡°í•© 5ê°œ
        // ì˜ˆ: #í•´ì¥ + ê²¨ìš¸ â†’ ["í•´ì¥", "ìˆ™ì·¨", "ì–¼í°í•œ", "ëœ¨ëˆí•œ", "ì•„ì¹¨"]
    ],
    "persona_insight": "{persona}ê°€ {current_season}ì— ì´ ê°€ê²Œë¥¼ ì°¾ëŠ” ì´ìœ ì™€ ê²€ìƒ‰ íŒ¨í„´",
    "insight": "{all_menus_str} ì¤‘ {current_season}ì— ê°€ì¥ ê²€ìƒ‰ëŸ‰ì´ ë§ì„ ë©”ë‰´ì™€ ì´ìœ "
}}

[ì¤‘ìš”]
1. ê³„ì ˆì„± í•„ìˆ˜ ë°˜ì˜ ({current_season})
2. ëª¨ë“  ë©”ë‰´ ê³¨ê³ ë£¨ í¬í•¨
3. {persona} íŠ¹ì„± ê³ ë ¤
"""
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.7,
            max_tokens=1000
        )
        result = json.loads(response.choices[0].message.content)
        result['season'] = current_season  # ê³„ì ˆ ì •ë³´ ì¶”ê°€
        
        logger.info(f"âœ… AI Materials ({current_season}):")
        logger.info(f"   - Actual menus: {result.get('actual_menus', [])}")
        logger.info(f"   - Seasonal keywords: {result.get('seasonal_keywords', [])}")
        return result
    except Exception as e:
        logger.error(f"âŒ AI Error: {e}")
        return None

# ---------------------------------------------------------
# [REVOLUTIONARY] ì „ëµì  í‚¤ì›Œë“œ ìƒì„± + ê²½ìŸì‚¬ ë¶„ì„
# ---------------------------------------------------------
def generate_and_validate_keywords(location, products, tags_input, materials, persona_text=""):
    """
    ì „ëµì  í‚¤ì›Œë“œ ìƒì„± + ê²½ìŸì‚¬ ë¶„ì„
    """
    if not materials:
        return create_empty_report()
    
    # 1. ì§€ì—­ íŒŒì‹±
    loc_hierarchy = parse_location_hierarchy(location)
    search_locations = loc_hierarchy['search_locations']
    
    if not search_locations:
        return create_empty_report()
    
    # 2. ë©”ë‰´ íŒŒì‹±
    menu_list = [m.strip() for m in products.split(",") if m.strip()]
    main_menu = menu_list[0] if menu_list else ""
    tags = [t.replace("#", "").strip() for t in tags_input.split() if t.strip()]
    
    current_season = materials.get('season', get_current_season())
    
    # 3. AI ì¬ë£Œ
    actual_menus = materials.get("actual_menus", menu_list[:5])
    expanded_menus = materials.get("expanded_menus", [])
    seasonal_keywords = materials.get("seasonal_keywords", [])
    
    # ëª¨ë“  ë©”ë‰´ í‚¤ì›Œë“œ ê²°í•©
    all_menu_keywords = []
    for menu in actual_menus[:5]:
        if menu and len(menu) >= 2:
            all_menu_keywords.append(menu)
    for menu in expanded_menus[:6]:
        if menu and menu not in all_menu_keywords:
            all_menu_keywords.append(menu)
    
    category_words = materials.get("category_words", ["ë§›ì§‘"])[:3]
    target_intents = materials.get("target_intents", ["ì ì‹¬"])[:5]
    situation_keywords = materials.get("situation_keywords", tags)[:5]
    purchase_triggers = materials.get("purchase_triggers", ["ì¶”ì²œ", "í›„ê¸°"])[:4]
    
    # 4. í‚¤ì›Œë“œ ìƒì„±
    keyword_pool = {
        'A_Core': [],
        'C_Target': [],
        'D_Situation': []
    }
    
    primary_loc = search_locations[0]
    
    # [Type A] ë©”ì¸ í‚¤ì›Œë“œ - ëª¨ë“  ë©”ë‰´ + ê³„ì ˆ
    for loc in search_locations[:2]:
        for menu in all_menu_keywords[:8]:
            keyword_pool['A_Core'].append(f"{loc} {menu}")
            keyword_pool['A_Core'].append(f"{loc} {menu} ë§›ì§‘")
        
        # ê³„ì ˆ í‚¤ì›Œë“œ
        for seasonal in seasonal_keywords[:3]:
            keyword_pool['A_Core'].append(f"{loc} {seasonal} {main_menu}")
            keyword_pool['A_Core'].append(f"{loc} {seasonal} ë§›ì§‘")
    
    # [Type C] íƒ€ê²Ÿ ë§ì¶¤ - ì‹¤ì œ ë©”ë‰´ ìš°ì„ 
    for intent in target_intents:
        for menu in actual_menus[:3]:
            keyword_pool['C_Target'].append(f"{primary_loc} {intent} {menu}")
        keyword_pool['C_Target'].append(f"{primary_loc} {intent} ë§›ì§‘")
    
    # [Type D] ìƒí™© + ê³„ì ˆ
    for situation in situation_keywords:
        keyword_pool['D_Situation'].append(f"{primary_loc} {situation}")
        keyword_pool['D_Situation'].append(f"{primary_loc} {situation} ë§›ì§‘")
        for menu in actual_menus[:2]:
            keyword_pool['D_Situation'].append(f"{primary_loc} {situation} {menu}")
    
    # 5. ì •ì œ
    for kw_type in keyword_pool:
        cleaned = []
        seen = set()
        for kwd in keyword_pool[kw_type]:
            sanitized = sanitize_keyword(kwd)
            if validate_keyword(sanitized) and sanitized not in seen:
                cleaned.append(sanitized)
                seen.add(sanitized)
        keyword_pool[kw_type] = cleaned
    
    # 6. ì¿¼í„° ì ìš©
    selected_candidates = []
    
    for kwd in keyword_pool['A_Core'][:6]:
        selected_candidates.append({"kwd": kwd, "type": "A_Core", "priority": 100})
    for kwd in keyword_pool['C_Target'][:7]:
        selected_candidates.append({"kwd": kwd, "type": "C_Target", "priority": 95})
    for kwd in keyword_pool['D_Situation'][:5]:
        selected_candidates.append({"kwd": kwd, "type": "D_Situation", "priority": 90})
    
    # 7. API ê²€ì¦
    validated_keywords = []
    if ADS_API_KEY:
        validated_keywords = validate_with_balanced_api(selected_candidates)
    
    # 8. í´ë°±
    validated_keywords = ensure_minimum_keywords(
        validated_keywords, selected_candidates, search_locations, 
        all_menu_keywords, main_menu
    )
    
    # 9. ê²½ìŸì‚¬ ë¶„ì„ (ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ)
    competitor_analysis = []
    top_keywords_for_analysis = []
    
    # ê²€ì¦ëœ í‚¤ì›Œë“œ ì¤‘ ê²€ìƒ‰ëŸ‰ ë†’ì€ ìˆœìœ¼ë¡œ 3ê°œ
    sorted_keywords = sorted(
        [kw for kw in validated_keywords if kw.get('type') in ['A_Core', 'C_Target']], 
        key=lambda x: x.get('volume', 0), 
        reverse=True
    )
    
    for kw_data in sorted_keywords[:3]:
        keyword = kw_data['keyword']
        comp_analysis = analyze_competitor_blogs(keyword)
        competitor_analysis.append({
            'keyword': keyword,
            'volume': kw_data.get('volume', 0),
            'analysis': comp_analysis
        })
        logger.info(f"ğŸ” Competitor analysis: {keyword} - {comp_analysis['total_posts']} posts")
    
    # 10. ê²°ê³¼ ë¶„ë¥˜ + ì „ëµ ìƒì„±
    final_report = classify_keywords_with_strategy(
        validated_keywords, materials, search_locations, 
        all_menu_keywords, tags, main_menu, 
        competitor_analysis, persona_text, current_season
    )
    
    return final_report

# ... (validate_with_balanced_api, ensure_minimum_keywords í•¨ìˆ˜ëŠ” ì´ì „ê³¼ ë™ì¼) ...
# ì´ì „ ì½”ë“œì—ì„œ ê°€ì ¸ì˜¤ê¸°

def validate_with_balanced_api(candidates):
    """íƒ€ì…ë³„ ê· í˜• API í˜¸ì¶œ"""
    validated = []
    api_call_count = 0
    MAX_API_CALLS = 5
    
    type_groups = {
        'A_Core': [c for c in candidates if c['type'] == 'A_Core'],
        'C_Target': [c for c in candidates if c['type'] == 'C_Target'],
        'D_Situation': [c for c in candidates if c['type'] == 'D_Situation']
    }
    
    batch_queue = []
    max_per_type = 2
    for i in range(max_per_type):
        for kw_type in ['A_Core', 'C_Target', 'D_Situation']:
            if i < len(type_groups[kw_type]):
                batch_queue.append(type_groups[kw_type][i])
    
    for kw_type in ['A_Core', 'C_Target', 'D_Situation']:
        remaining = type_groups[kw_type][max_per_type:]
        batch_queue.extend(remaining[:2])
    
    for i in range(0, len(batch_queue), 5):
        if api_call_count >= MAX_API_CALLS:
            break
        
        chunk = batch_queue[i:i+5]
        hint_str = ",".join([c['kwd'].replace(" ", "") for c in chunk])
        
        try:
            headers = get_header("GET", "/keywordstool", ADS_API_KEY, ADS_SECRET_KEY, CUSTOMER_ID)
            params = {"hintKeywords": hint_str, "showDetail": "1"}
            
            res = requests.get(ADS_BASE_URL + "/keywordstool", params=params, headers=headers, timeout=5)
            
            if res.status_code == 200:
                api_data = res.json().get("keywordList", [])
                
                for item in api_data:
                    rel_kwd = item.get('relKeyword', '')
                    pc = int(item.get('monthlyPcQcCnt', 0)) if str(item.get('monthlyPcQcCnt', 0)).isdigit() else 0
                    mo = int(item.get('monthlyMobileQcCnt', 0)) if str(item.get('monthlyMobileQcCnt', 0)).isdigit() else 0
                    comp_idx = item.get('compIdx', 'low')
                    
                    total_volume = pc + mo
                    
                    if total_volume >= 10:
                        matched = None
                        for c in chunk:
                            c_clean = c['kwd'].replace(" ", "")
                            rel_clean = rel_kwd.replace(" ", "")
                            if c_clean in rel_clean or rel_clean in c_clean:
                                matched = c
                                break
                        
                        if not matched:
                            matched = {"type": "E_Related", "priority": 50}
                        
                        comp_score = {'low': 100, 'medium': 50, 'high': 20}.get(comp_idx, 50)
                        score = (total_volume * 0.5) + comp_score + matched.get('priority', 0)
                        
                        validated.append({
                            "keyword": rel_kwd,
                            "volume": total_volume,
                            "pc": pc,
                            "mobile": mo,
                            "competition": comp_idx,
                            "type": matched['type'],
                            "score": score
                        })
            
            api_call_count += 1
            time.sleep(0.2)
            
        except Exception as e:
            logger.error(f"API Exception: {str(e)}")
            api_call_count += 1
    
    return validated

def ensure_minimum_keywords(validated, all_candidates, locations, menus, main_menu):
    """íƒ€ì…ë³„ ìµœì†Œ ê°œìˆ˜ ë³´ì¥"""
    type_counts = {}
    for kw in validated:
        kw_type = kw['type']
        type_counts[kw_type] = type_counts.get(kw_type, 0) + 1
    
    min_targets = {
        'A_Core': 4,
        'C_Target': 5,
        'D_Situation': 4
    }
    
    existing_kwds = set([kw['keyword'] for kw in validated])
    
    for kw_type, min_count in min_targets.items():
        current_count = type_counts.get(kw_type, 0)
        shortage = min_count - current_count
        
        if shortage > 0:
            type_candidates = [c for c in all_candidates if c['type'] == kw_type and c['kwd'] not in existing_kwds]
            
            for c in type_candidates[:shortage]:
                base_volume = 200
                if locations[0] in c['kwd']:
                    base_volume = 250
                
                if kw_type == 'C_Target':
                    base_volume = int(base_volume * 0.7)
                elif kw_type == 'D_Situation':
                    base_volume = int(base_volume * 0.5)
                
                validated.append({
                    "keyword": c['kwd'],
                    "volume": base_volume,
                    "pc": int(base_volume * 0.3),
                    "mobile": int(base_volume * 0.7),
                    "competition": "low",
                    "type": kw_type,
                    "score": c['priority'],
                    "is_estimated": True
                })
                existing_kwds.add(c['kwd'])
    
    return validated

# ---------------------------------------------------------
# [NEW] ì „ëµì  ë¶„ë¥˜ í•¨ìˆ˜
# ---------------------------------------------------------
def classify_keywords_with_strategy(validated_keywords, materials, locations, menus, tags, main_menu, competitor_analysis, persona_text, season):
    """í‚¤ì›Œë“œ ë¶„ë¥˜ + ì „ëµ ìƒì„±"""
    
    final_report = {
        "season": season,
        "persona_insight": materials.get("persona_insight", ""),
        "insight": materials.get("insight", ""),
        "main_keywords": [],
        "detail_keywords": [],
        "related_keywords": [],
        "competitor_analysis": competitor_analysis,
        "strategic_recommendations": [],
        "content_ideas": [],
        "action_plan": {}
    }
    
    # ì¤‘ë³µ ì œê±°
    unique_validated = {}
    for kw in validated_keywords:
        key = kw['keyword']
        if key not in unique_validated or unique_validated[key]['score'] < kw['score']:
            unique_validated[key] = kw
    
    validated_keywords = list(unique_validated.values())
    validated_keywords.sort(key=lambda x: x['score'], reverse=True)
    
    # ë¶„ë¥˜
    for kw in validated_keywords:
        kw_type = kw['type']
        if kw_type in ['A_Core', 'B_Local']:
            final_report['main_keywords'].append(kw)
        elif kw_type in ['C_Target', 'D_Situation']:
            final_report['detail_keywords'].append(kw)
        elif kw_type == 'E_Related':
            final_report['related_keywords'].append(kw)
    
    final_report['main_keywords'] = final_report['main_keywords'][:10]
    final_report['detail_keywords'] = final_report['detail_keywords'][:12]
    final_report['related_keywords'] = final_report['related_keywords'][:5]
    
    # ì „ëµ ì¶”ì²œ ìƒì„±
    recommendations = []
    
    # 1. ê²½ìŸ ê°•ë„ ê¸°ë°˜ ì „ëµ
    if competitor_analysis:
        top_comp = competitor_analysis[0]
        comp_level = top_comp['analysis']['competition_level']
        
        if comp_level == 'low':
            recommendations.append({
                'priority': 'HIGH',
                'strategy': 'ë¸”ë¡œê·¸ ì„ ì  ì „ëµ',
                'description': f"'{top_comp['keyword']}' í‚¤ì›Œë“œëŠ” ê²½ìŸì´ ì•½í•©ë‹ˆë‹¤. ë¸”ë¡œê·¸ 20-30ê°œë§Œìœ¼ë¡œ ìƒìœ„ ë…¸ì¶œ ê°€ëŠ¥!",
                'action': f"1ê°œì›”ê°„ ì£¼ 2-3íšŒ ë¸”ë¡œê·¸ í¬ìŠ¤íŒ… ì§‘ì¤‘",
                'expected_result': '3ê°œì›” ë‚´ ë„¤ì´ë²„ ê²€ìƒ‰ 1í˜ì´ì§€ ì§„ì…'
            })
        elif comp_level == 'medium':
            recommendations.append({
                'priority': 'MEDIUM',
                'strategy': 'ë³µí•© ì±„ë„ ì „ëµ',
                'description': f"'{top_comp['keyword']}'ëŠ” ì¤‘ê°„ ê²½ìŸì…ë‹ˆë‹¤. ë¸”ë¡œê·¸ + ì¸ìŠ¤íƒ€ê·¸ë¨ + ë¦¬ë·° ê´€ë¦¬ ë³‘í–‰ í•„ìš”",
                'action': 'ë¸”ë¡œê·¸(ì£¼ 2íšŒ) + ì¸ìŠ¤íƒ€(ì£¼ 5íšŒ) + ê³ ê° ë¦¬ë·° ìœ ë„',
                'expected_result': '4-6ê°œì›” ë‚´ ìƒìœ„ 5ìœ„ê¶Œ ì§„ì…'
            })
        else:
            recommendations.append({
                'priority': 'STRATEGIC',
                'strategy': 'ë¡±í…Œì¼ í‚¤ì›Œë“œ ê³µëµ',
                'description': f"'{top_comp['keyword']}'ëŠ” ê²½ìŸì´ ì¹˜ì—´í•©ë‹ˆë‹¤. ì„¸ë¶€ í‚¤ì›Œë“œë¡œ ìš°íšŒ ê³µëµ ì¶”ì²œ",
                'action': f"'{final_report['detail_keywords'][0]['keyword']}' ê°™ì€ ë¡±í…Œì¼ í‚¤ì›Œë“œ ì§‘ì¤‘",
                'expected_result': '2-3ê°œì›” ë‚´ í‹ˆìƒˆ í‚¤ì›Œë“œ ìƒìœ„ ë…¸ì¶œ'
            })
    
    # 2. ê³„ì ˆ ê¸°ë°˜ ì „ëµ
    recommendations.append({
        'priority': 'SEASONAL',
        'strategy': f'{season} ì‹œì¦Œ ë§ˆì¼€íŒ…',
        'description': f"ì§€ê¸ˆì€ {season}ì…ë‹ˆë‹¤. ê³„ì ˆ í‚¤ì›Œë“œë¥¼ í™œìš©í•œ ì½˜í…ì¸  ì œì‘ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.",
        'action': f"{season} ê´€ë ¨ ë¸”ë¡œê·¸/ì¸ìŠ¤íƒ€ ì½˜í…ì¸  ì§‘ì¤‘ (ì˜ˆ: '{season} {main_menu}')",
        'expected_result': f'{season} ê¸°ê°„(3ê°œì›”) ë™ì•ˆ ìœ ì… 30% ì¦ê°€ ì˜ˆìƒ'
    })
    
    # 3. íƒ€ê²Ÿì¸µ ê¸°ë°˜ ì „ëµ
    if persona_text:
        recommendations.append({
            'priority': 'TARGET',
            'strategy': f'{persona_text} ë§ì¶¤ ì½˜í…ì¸ ',
            'description': materials.get('persona_insight', 'íƒ€ê²Ÿ ê³ ê°ì˜ ê²€ìƒ‰ íŒ¨í„´ì„ ë°˜ì˜í•œ ì „ëµ'),
            'action': f"íƒ€ê²Ÿì¸µì´ ë§ì´ ê²€ìƒ‰í•˜ëŠ” '{final_report['detail_keywords'][0]['keyword'] if final_report['detail_keywords'] else main_menu}' í‚¤ì›Œë“œ ì§‘ì¤‘",
            'expected_result': 'ì „í™˜ìœ¨ 20% í–¥ìƒ ê¸°ëŒ€'
        })
    
    final_report['strategic_recommendations'] = recommendations
    
    # ì‹¤í–‰ ê³„íš
    total_keywords = len(final_report['main_keywords']) + len(final_report['detail_keywords'])
    
    final_report['action_plan'] = {
        'month_1': {
            'focus': 'ê¸°ë°˜ êµ¬ì¶•',
            'actions': [
                f"ë¸”ë¡œê·¸ í¬ìŠ¤íŒ… 8-10ê°œ ì‘ì„± (ë©”ì¸ í‚¤ì›Œë“œ {len(final_report['main_keywords'][:3])}ê°œ í¬í•¨)",
                "ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì •ë³´ ìµœì í™”",
                "ê³ ê° ë¦¬ë·° 5ê°œ ì´ìƒ í™•ë³´"
            ],
            'expected': 'ë„¤ì´ë²„ ê²€ìƒ‰ ë…¸ì¶œ ì‹œì‘'
        },
        'month_2': {
            'focus': 'í™•ì¥ ë° ê°•í™”',
            'actions': [
                "ë¸”ë¡œê·¸ í¬ìŠ¤íŒ… ì¶”ê°€ 10ê°œ (ì„¸ë¶€ í‚¤ì›Œë“œ í¬í•¨)",
                "ì¸ìŠ¤íƒ€ê·¸ë¨ ì—°ë™ ì‹œì‘ (ì£¼ 3-5íšŒ)",
                "ê¸°ì¡´ í¬ìŠ¤íŒ… ì—…ë°ì´íŠ¸"
            ],
            'expected': 'ê²€ìƒ‰ ìˆœìœ„ 10-20ìœ„ê¶Œ ì§„ì…'
        },
        'month_3': {
            'focus': 'ìµœì í™” ë° ìœ ì§€',
            'actions': [
                "ìƒìœ„ ë…¸ì¶œ í‚¤ì›Œë“œ ì§‘ì¤‘ ê´€ë¦¬",
                "ë¦¬ë·° ê´€ë¦¬ ë° ë‹µê¸€",
                "ê³„ì ˆë³„ ì½˜í…ì¸  ì—…ë°ì´íŠ¸"
            ],
            'expected': 'ëª©í‘œ í‚¤ì›Œë“œ ìƒìœ„ 5-10ìœ„ ì•ˆì •í™”'
        }
    }
    
    # ì½˜í…ì¸  ì•„ì´ë””ì–´ (êµ¬ì²´ì ìœ¼ë¡œ)
    ideas = []
    
    if final_report['main_keywords']:
        top_kw = final_report['main_keywords'][0]
        ideas.append({
            'type': 'SEO ë¸”ë¡œê·¸',
            'title': f"\"{top_kw['keyword']} BEST 5 - í˜„ì§€ì¸ì´ ì¶”ì²œí•˜ëŠ” ì§„ì§œ ë§›ì§‘\"",
            'reason': f"ì›” {top_kw['volume']:,}ê±´ ê²€ìƒ‰ë˜ëŠ” ë©”ì¸ í‚¤ì›Œë“œ ê³µëµ",
            'content_guide': f"1. ìš°ë¦¬ ê°€ê²Œ ì†Œê°œ (ì‚¬ì§„ 5ì¥+), 2. ë©”ë‰´ ë¦¬ë·°, 3. ê°€ê²©/ì£¼ì°¨ ì •ë³´, 4. ë°©ë¬¸ í›„ê¸°"
        })
    
    if final_report['detail_keywords'] and tags:
        detail_kw = final_report['detail_keywords'][0]
        ideas.append({
            'type': 'ìƒí™© ê³µê° ì½˜í…ì¸ ',
            'title': f"\"{tags[0]} ë•Œ ìƒê°ë‚˜ëŠ” {locations[0]} {main_menu}, ì—¬ê¸° ê°€ì„¸ìš”\"",
            'reason': f"íŠ¹ì • ìƒí™© ê²€ìƒ‰ ê³ ê° ì „í™˜ìœ¨ ë†’ìŒ",
            'content_guide': f"1. ìƒí™© ê³µê° ìŠ¤í† ë¦¬í…”ë§, 2. ìš°ë¦¬ ê°€ê²Œê°€ ë”±ì¸ ì´ìœ , 3. ì‹¤ì œ ë°©ë¬¸ ì‚¬ì§„, 4. ê¿€íŒ"
        })
    
    if season:
        ideas.append({
            'type': 'ê³„ì ˆ ì½˜í…ì¸ ',
            'title': f"\"{season}ì— ë” ë§›ìˆëŠ” {main_menu}, {locations[0]}ì—ì„œ ë¨¹ì–´ì•¼ í•˜ëŠ” ì´ìœ \"",
            'reason': f"{season} ì‹œì¦Œ íŠ¹ìˆ˜ í™œìš©",
            'content_guide': f"1. {season} íŠ¹ì„±ê³¼ ë©”ë‰´ ì—°ê²°, 2. ê³„ì ˆ í•œì • ë©”ë‰´ ê°•ì¡°, 3. ë¶„ìœ„ê¸° ì‚¬ì§„"
        })
    
    if competitor_analysis:
        comp = competitor_analysis[0]
        ideas.append({
            'type': 'ì°¨ë³„í™” ì½˜í…ì¸ ',
            'title': f"\"{comp['keyword']} ìˆ¨ì€ ë§›ì§‘ - ë¸”ë¡œê·¸ì— ì•ˆ ë‚˜ì˜¨ ì§„ì§œ ë§›ì§‘\"",
            'reason': f"ê²½ìŸ í‚¤ì›Œë“œ ìš°íšŒ ê³µëµ",
            'content_guide': "1. 'ìˆ¨ì€ ë§›ì§‘' ì»¨ì…‰, 2. ë‹¤ë¥¸ ê³³ê³¼ ì°¨ë³„ì , 3. ë‹¨ê³¨ ì¸í„°ë·°"
        })
    
    final_report['content_ideas'] = ideas[:4]
    
    return final_report

def create_empty_report():
    """ë¹ˆ ë¦¬í¬íŠ¸"""
    return {
        "season": get_current_season(),
        "insight": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±",
        "main_keywords": [],
        "detail_keywords": [],
        "related_keywords": [],
        "competitor_analysis": [],
        "strategic_recommendations": [],
        "content_ideas": [],
        "action_plan": {}
    }